W1224 11:17:05.100000 14014 site-packages/torch/distributed/run.py:793] 
W1224 11:17:05.100000 14014 site-packages/torch/distributed/run.py:793] *****************************************
W1224 11:17:05.100000 14014 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1224 11:17:05.100000 14014 site-packages/torch/distributed/run.py:793] *****************************************
[rank:0] torch version 2.3.1 or higher --> OK
[rank:0] transformers version 4.46.2 or higher --> OK
> Total parameters in model: 1,235,814,400
> World size: 8
> IR: IR_Anal.SEQUENTIAL
> GAS: 1
> GBS: 32
> MBS: 32
> TP: 2
> DP: 1
> PP: 4
Available GPUs per server: 8
GPU mode is used.
GPU mode is used.
GPU mode is used.
GPU mode is used.
GPU mode is used.
GPU mode is used.
GPU mode is used.
GPU mode is used.
> [rank:3] pp group:DeviceMesh('cuda', [1, 3, 5, 7], mesh_dim_names=('pp',)), tp group:DeviceMesh('cuda', [2, 3], mesh_dim_names=('tp',)), dp group:DeviceMesh('cuda', [3], mesh_dim_names=('dp',))
> World Size: 8
> Pipeline Parallel Size: 4
> Tensor Parallel Size: 2
>> ir_analyze: IR_Anal.SEQUENTIAL
> [rank:0] pp group:DeviceMesh('cuda', [0, 2, 4, 6], mesh_dim_names=('pp',)), tp group:DeviceMesh('cuda', [0, 1], mesh_dim_names=('tp',)), dp group:DeviceMesh('cuda', [0], mesh_dim_names=('dp',))
 ----------------------------------
stage2rank = {0: [0, 1], 1: [2, 3], 2: [4, 5], 3: [6, 7]}
 ----------------------------------
> [rank:5] pp group:DeviceMesh('cuda', [1, 3, 5, 7], mesh_dim_names=('pp',)), tp group:DeviceMesh('cuda', [4, 5], mesh_dim_names=('tp',)), dp group:DeviceMesh('cuda', [5], mesh_dim_names=('dp',))
>>> Using GPU ... cuda:0
>> model class name: LlamaForCausalLM
>> split method: llama-tp-split
SEQUENTIAL mode >> [rank:0, local_world_size:8]
> [rank:4] pp group:DeviceMesh('cuda', [0, 2, 4, 6], mesh_dim_names=('pp',)), tp group:DeviceMesh('cuda', [4, 5], mesh_dim_names=('tp',)), dp group:DeviceMesh('cuda', [4], mesh_dim_names=('dp',))
> [rank:7] pp group:DeviceMesh('cuda', [1, 3, 5, 7], mesh_dim_names=('pp',)), tp group:DeviceMesh('cuda', [6, 7], mesh_dim_names=('tp',)), dp group:DeviceMesh('cuda', [7], mesh_dim_names=('dp',))
> [rank:2] pp group:DeviceMesh('cuda', [0, 2, 4, 6], mesh_dim_names=('pp',)), tp group:DeviceMesh('cuda', [2, 3], mesh_dim_names=('tp',)), dp group:DeviceMesh('cuda', [2], mesh_dim_names=('dp',))
> [rank:1] pp group:DeviceMesh('cuda', [1, 3, 5, 7], mesh_dim_names=('pp',)), tp group:DeviceMesh('cuda', [0, 1], mesh_dim_names=('tp',)), dp group:DeviceMesh('cuda', [1], mesh_dim_names=('dp',))
> [rank:6] pp group:DeviceMesh('cuda', [0, 2, 4, 6], mesh_dim_names=('pp',)), tp group:DeviceMesh('cuda', [6, 7], mesh_dim_names=('tp',)), dp group:DeviceMesh('cuda', [6], mesh_dim_names=('dp',))
 num_stage: 4
 num_blocks = 18
 layers_per_stage = [0, 4, 9, 13, 18]
 stage_layers = [[0, 1, 2, 3], [4, 5, 6, 7, 8], [9, 10, 11, 12, 13], [14, 15, 16, 17]]
 ------------------------------------------------------------
  rank:0,  first metadata_range: [(0, 'model_layers_3_mlp_down_proj'), (1, 'model_layers_8_mlp_down_proj'), (2, 'model_layers_13_mlp_down_proj'), (3, 'model_layers_15_mlp_down_proj')]
 ------------------------------------------------------------
>>> Using GPU ... cuda:3
>> model class name: LlamaForCausalLM
>> split method: llama-tp-split
SEQUENTIAL mode >> [rank:3, local_world_size:8]
[rank3]:[W1224 11:17:12.265371710 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
>>> Using GPU ... cuda:5
>> model class name: LlamaForCausalLM
>> split method: llama-tp-split
SEQUENTIAL mode >> [rank:5, local_world_size:8]
[rank5]:[W1224 11:17:12.437216425 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 5]  using GPU 5 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
>>> Using GPU ... cuda:4
>> model class name: LlamaForCausalLM
>> split method: llama-tp-split
SEQUENTIAL mode >> [rank:4, local_world_size:8]
[rank4]:[W1224 11:17:12.549256952 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 4]  using GPU 4 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
 ------------------------------------------------------------
  rank:0,  second metadata_range: [(0, 'submod_0'), (1, 'submod_1'), (2, 'submod_2'), (3, 'submod_3')]
 ------------------------------------------------------------
>> ------------------ FX graph --------------------------------
n.op:placeholder, n.name:input_ids, n.target:input_ids, n.args:(), n.all_input_nodes:[]
n.op:call_module, n.name:submod_0, n.target:submod_0, n.args:(input_ids,), n.all_input_nodes:[input_ids]
n.op:call_function, n.name:getitem, n.target:<built-in function getitem>, n.args:(submod_0, 0), n.all_input_nodes:[submod_0]
n.op:call_function, n.name:getitem_1, n.target:<built-in function getitem>, n.args:(submod_0, 1), n.all_input_nodes:[submod_0]
n.op:call_function, n.name:getitem_2, n.target:<built-in function getitem>, n.args:(submod_0, 2), n.all_input_nodes:[submod_0]
n.op:call_function, n.name:getitem_3, n.target:<built-in function getitem>, n.args:(submod_0, 3), n.all_input_nodes:[submod_0]
n.op:call_function, n.name:getitem_4, n.target:<built-in function getitem>, n.args:(submod_0, 4), n.all_input_nodes:[submod_0]
n.op:call_module, n.name:submod_1, n.target:submod_1, n.args:(getitem, getitem_1, getitem_2, getitem_3, getitem_4), n.all_input_nodes:[getitem, getitem_1, getitem_2, getitem_3, getitem_4]
n.op:call_function, n.name:getitem_5, n.target:<built-in function getitem>, n.args:(submod_1, 0), n.all_input_nodes:[submod_1]
n.op:call_function, n.name:getitem_6, n.target:<built-in function getitem>, n.args:(submod_1, 1), n.all_input_nodes:[submod_1]
n.op:call_module, n.name:submod_2, n.target:submod_2, n.args:(getitem_5, getitem_6, getitem_2, getitem_3, getitem_4), n.all_input_nodes:[getitem_5, getitem_6, getitem_2, getitem_3, getitem_4]
n.op:call_function, n.name:getitem_7, n.target:<built-in function getitem>, n.args:(submod_2, 0), n.all_input_nodes:[submod_2]
n.op:call_function, n.name:getitem_8, n.target:<built-in function getitem>, n.args:(submod_2, 1), n.all_input_nodes:[submod_2]
n.op:call_module, n.name:submod_3, n.target:submod_3, n.args:(getitem_7, getitem_8, getitem_2, getitem_3, getitem_4), n.all_input_nodes:[getitem_7, getitem_8, getitem_2, getitem_3, getitem_4]
n.op:output, n.name:output, n.target:output, n.args:({'logits': submod_3},), n.all_input_nodes:[submod_3]
>> ------------------------------------------------------------
>>> Using GPU ... cuda:2
>> model class name: LlamaForCausalLM
>> split method: llama-tp-split
SEQUENTIAL mode >> [rank:2, local_world_size:8]
>>> Using GPU ... cuda:7
>> model class name: LlamaForCausalLM
>> split method: llama-tp-split
SEQUENTIAL mode >> [rank:7, local_world_size:8]
[rank7]:[W1224 11:17:12.699153479 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 7]  using GPU 7 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank2]:[W1224 11:17:12.703266618 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
>>> Using GPU ... cuda:1
>> model class name: LlamaForCausalLM
>> split method: llama-tp-split
SEQUENTIAL mode >> [rank:1, local_world_size:8]
[rank1]:[W1224 11:17:12.721424027 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
>>> Using GPU ... cuda:6
>> model class name: LlamaForCausalLM
>> split method: llama-tp-split
SEQUENTIAL mode >> [rank:6, local_world_size:8]
[rank6]:[W1224 11:17:12.738139262 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 6]  using GPU 6 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
 ### Rank:0, name:submod_0, move submod_0 to cuda:0
 # rank = 0, metadata_range:[(0, 'submod_0'), (1, 'submod_1'), (2, 'submod_2'), (3, 'submod_3')]
-- node.op:placeholder, node.name:input_ids, node.target:input_ids, node.args:(), node.all_input_nodes:[]
-- node.op:call_module, node.name:submod_0, node.target:submod_0, node.args:(input_ids,), node.all_input_nodes:[input_ids]
-- node.op:call_function, node.name:getitem, node.target:<built-in function getitem>, node.args:(submod_0, 0), node.all_input_nodes:[submod_0]
-- node.op:call_function, node.name:getitem_1, node.target:<built-in function getitem>, node.args:(submod_0, 1), node.all_input_nodes:[submod_0]
-- node.op:call_function, node.name:getitem_2, node.target:<built-in function getitem>, node.args:(submod_0, 2), node.all_input_nodes:[submod_0]
-- node.op:call_function, node.name:getitem_3, node.target:<built-in function getitem>, node.args:(submod_0, 3), node.all_input_nodes:[submod_0]
-- node.op:call_function, node.name:getitem_4, node.target:<built-in function getitem>, node.args:(submod_0, 4), node.all_input_nodes:[submod_0]
-- node.op:call_module, node.name:submod_1, node.target:submod_1, node.args:(getitem, getitem_1, getitem_2, getitem_3, getitem_4), node.all_input_nodes:[getitem, getitem_1, getitem_2, getitem_3, getitem_4]
-- node.op:call_function, node.name:getitem_5, node.target:<built-in function getitem>, node.args:(submod_1, 0), node.all_input_nodes:[submod_1]
-- node.op:call_function, node.name:getitem_6, node.target:<built-in function getitem>, node.args:(submod_1, 1), node.all_input_nodes:[submod_1]
-- node.op:call_module, node.name:submod_2, node.target:submod_2, node.args:(getitem_5, getitem_6, getitem_2, getitem_3, getitem_4), node.all_input_nodes:[getitem_5, getitem_6, getitem_2, getitem_3, getitem_4]
-- node.op:call_function, node.name:getitem_7, node.target:<built-in function getitem>, node.args:(submod_2, 0), node.all_input_nodes:[submod_2]
-- node.op:call_function, node.name:getitem_8, node.target:<built-in function getitem>, node.args:(submod_2, 1), node.all_input_nodes:[submod_2]
-- node.op:call_module, node.name:submod_3, node.target:submod_3, node.args:(getitem_7, getitem_8, getitem_2, getitem_3, getitem_4), node.all_input_nodes:[getitem_7, getitem_8, getitem_2, getitem_3, getitem_4]
-- node.op:output, node.name:output, node.target:output, node.args:({'logits': submod_3},), node.all_input_nodes:[submod_3]
 ========= getitem_dic =========
 --- key:getitem, values:('submod_0', 0)
 --- key:getitem_1, values:('submod_0', 1)
 --- key:getitem_2, values:('submod_0', 2)
 --- key:getitem_3, values:('submod_0', 3)
 --- key:getitem_4, values:('submod_0', 4)
 --- key:getitem_5, values:('submod_1', 0)
 --- key:getitem_6, values:('submod_1', 1)
 --- key:getitem_7, values:('submod_2', 0)
 --- key:getitem_8, values:('submod_2', 1)
 ===============================
 ***** stage:3 >>  from_:getitem_7, to_:submod_3
 ***** stage:2 >>  from_:getitem_5, to_:submod_2
 ***** stage:1 >>  from_:getitem, to_:submod_1
 ### Rank:0, clean_module_memory ...
[rank:0, local_rank:0] SEQUENTIAL MODE PROCESSING ...
[rank0]:[W1224 11:17:13.480937575 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
 num_stage: 4
 num_blocks = 18
 layers_per_stage = [0, 4, 9, 13, 18]
 stage_layers = [[0, 1, 2, 3], [4, 5, 6, 7, 8], [9, 10, 11, 12, 13], [14, 15, 16, 17]]
 ------------------------------------------------------------
  rank:1,  second metadata_range: [(0, 'submod_0'), (1, 'submod_1'), (2, 'submod_2'), (3, 'submod_3')]
 ------------------------------------------------------------
 ### Rank:1, name:submod_0, move submod_0 to cuda:1
 ***** stage:3 >>  from_:getitem_7, to_:submod_3
 ***** stage:2 >>  from_:getitem_5, to_:submod_2
 ***** stage:1 >>  from_:getitem, to_:submod_1
 ### Rank:1, clean_module_memory ...
[rank:1, local_rank:1] SEQUENTIAL MODE PROCESSING ...
 ------------------------------------------------------------
  rank:2,  second metadata_range: [(0, 'submod_0'), (1, 'submod_1'), (2, 'submod_2'), (3, 'submod_3')]
 ------------------------------------------------------------
 ### Rank:2, name:submod_1, move submod_1 to cuda:2
 ***** stage:3 >>  from_:getitem_7, to_:submod_3
 ***** stage:2 >>  from_:getitem_5, to_:submod_2
 ***** stage:1 >>  from_:getitem, to_:submod_1
 ### Rank:2, clean_module_memory ...
[rank:2, local_rank:2] SEQUENTIAL MODE PROCESSING ...
 ------------------------------------------------------------
  rank:3,  second metadata_range: [(0, 'submod_0'), (1, 'submod_1'), (2, 'submod_2'), (3, 'submod_3')]
 ------------------------------------------------------------
 ### Rank:3, name:submod_1, move submod_1 to cuda:3
 ***** stage:3 >>  from_:getitem_7, to_:submod_3
 ***** stage:2 >>  from_:getitem_5, to_:submod_2
 ***** stage:1 >>  from_:getitem, to_:submod_1
 ### Rank:3, clean_module_memory ...
[rank:3, local_rank:3] SEQUENTIAL MODE PROCESSING ...
 ------------------------------------------------------------
  rank:4,  second metadata_range: [(0, 'submod_0'), (1, 'submod_1'), (2, 'submod_2'), (3, 'submod_3')]
 ------------------------------------------------------------
 ### Rank:4, name:submod_2, move submod_2 to cuda:4
 ***** stage:3 >>  from_:getitem_7, to_:submod_3
 ***** stage:2 >>  from_:getitem_5, to_:submod_2
 ***** stage:1 >>  from_:getitem, to_:submod_1
 ### Rank:4, clean_module_memory ...
[rank:4, local_rank:4] SEQUENTIAL MODE PROCESSING ...
 ------------------------------------------------------------
  rank:5,  second metadata_range: [(0, 'submod_0'), (1, 'submod_1'), (2, 'submod_2'), (3, 'submod_3')]
 ------------------------------------------------------------
 ### Rank:5, name:submod_2, move submod_2 to cuda:5
 ***** stage:3 >>  from_:getitem_7, to_:submod_3
 ***** stage:2 >>  from_:getitem_5, to_:submod_2
 ***** stage:1 >>  from_:getitem, to_:submod_1
 ### Rank:5, clean_module_memory ...
[rank:5, local_rank:5] SEQUENTIAL MODE PROCESSING ...
 ------------------------------------------------------------
  rank:6,  second metadata_range: [(0, 'submod_0'), (1, 'submod_1'), (2, 'submod_2'), (3, 'submod_3')]
 ------------------------------------------------------------
 ### Rank:6, name:submod_3, move submod_3 to cuda:6
 ***** stage:3 >>  from_:getitem_7, to_:submod_3
 ***** stage:2 >>  from_:getitem_5, to_:submod_2
 ***** stage:1 >>  from_:getitem, to_:submod_1
 ### Rank:6, clean_module_memory ...
[rank:6, local_rank:6] SEQUENTIAL MODE PROCESSING ...
 ------------------------------------------------------------
  rank:7,  second metadata_range: [(0, 'submod_0'), (1, 'submod_1'), (2, 'submod_2'), (3, 'submod_3')]
 ------------------------------------------------------------
 ### Rank:7, name:submod_3, move submod_3 to cuda:7
 ***** stage:3 >>  from_:getitem_7, to_:submod_3
 ***** stage:2 >>  from_:getitem_5, to_:submod_2
 ***** stage:1 >>  from_:getitem, to_:submod_1
 ### Rank:7, clean_module_memory ...
[rank:7, local_rank:7] SEQUENTIAL MODE PROCESSING ...
 >> rank:7 -----------------------------------------------
rank: 7, #### last layer id:15
 >> rank:7 -----------------------------------------------
>>>> self.tpl.tp_mesh.size(): 2
>model_layers_14_self_attn_q_proj ==> node.args[3]:32
> >model_layers_14_self_attn_q_proj ==> node.args[3]:16
>>model_layers_14_self_attn_k_proj ===> node.args[3]:8
>> >> model_layers_14_self_attn_k_proj ===> node.args[3]:4
>>>model_layers_14_self_attn_v_proj ===> node.args[3]:8
>>> >>>model_layers_14_self_attn_v_proj ===> node.args[3]:4
>model_layers_15_self_attn_q_proj ==> node.args[3]:32
> >model_layers_15_self_attn_q_proj ==> node.args[3]:16
>>model_layers_15_self_attn_k_proj ===> node.args[3]:8
>> >> model_layers_15_self_attn_k_proj ===> node.args[3]:4
>>>model_layers_15_self_attn_v_proj ===> node.args[3]:8
>>> >>>model_layers_15_self_attn_v_proj ===> node.args[3]:4
 >> rank:5 ----------------------------------------------- >> rank:2 -----------------------------------------------
 >> rank:3 -----------------------------------------------
rank: 5, #### last layer id:13
rank: 2, #### last layer id:8
rank: 3, #### last layer id:8
 >> rank:5 -----------------------------------------------
 >> rank:2 -----------------------------------------------
 >> rank:3 -----------------------------------------------

>>>> self.tpl.tp_mesh.size(): 2>>>> self.tpl.tp_mesh.size(): 2>>>> self.tpl.tp_mesh.size(): 2


>model_layers_9_self_attn_q_proj ==> node.args[3]:32
>model_layers_4_self_attn_q_proj ==> node.args[3]:32
>model_layers_4_self_attn_q_proj ==> node.args[3]:32
> >model_layers_9_self_attn_q_proj ==> node.args[3]:16
>>model_layers_9_self_attn_k_proj ===> node.args[3]:8
>> >> model_layers_9_self_attn_k_proj ===> node.args[3]:4
>>>model_layers_9_self_attn_v_proj ===> node.args[3]:8> >model_layers_4_self_attn_q_proj ==> node.args[3]:16
> >model_layers_4_self_attn_q_proj ==> node.args[3]:16

>>> >>>model_layers_9_self_attn_v_proj ===> node.args[3]:4>>model_layers_4_self_attn_k_proj ===> node.args[3]:8
>>model_layers_4_self_attn_k_proj ===> node.args[3]:8 >> rank:4 -----------------------------------------------

>model_layers_10_self_attn_q_proj ==> node.args[3]:32

>> >> model_layers_4_self_attn_k_proj ===> node.args[3]:4>> >> model_layers_4_self_attn_k_proj ===> node.args[3]:4rank: 4, #### last layer id:13

> >model_layers_10_self_attn_q_proj ==> node.args[3]:16
>>>model_layers_4_self_attn_v_proj ===> node.args[3]:8>>>model_layers_4_self_attn_v_proj ===> node.args[3]:8 >> rank:0 -----------------------------------------------
 >> rank:4 -----------------------------------------------

 >> rank:1 -----------------------------------------------
>>model_layers_10_self_attn_k_proj ===> node.args[3]:8
>>> >>>model_layers_4_self_attn_v_proj ===> node.args[3]:4>>> >>>model_layers_4_self_attn_v_proj ===> node.args[3]:4
rank: 0, #### last layer id:3


>> >> model_layers_10_self_attn_k_proj ===> node.args[3]:4rank: 1, #### last layer id:3

>>>> self.tpl.tp_mesh.size(): 2
 >> rank:0 ----------------------------------------------->>>model_layers_10_self_attn_v_proj ===> node.args[3]:8
 >> rank:1 -----------------------------------------------
>model_layers_5_self_attn_q_proj ==> node.args[3]:32
>model_layers_5_self_attn_q_proj ==> node.args[3]:32


>>> >>>model_layers_10_self_attn_v_proj ===> node.args[3]:4
> >model_layers_5_self_attn_q_proj ==> node.args[3]:16> >model_layers_5_self_attn_q_proj ==> node.args[3]:16

>>model_layers_5_self_attn_k_proj ===> node.args[3]:8>>model_layers_5_self_attn_k_proj ===> node.args[3]:8
>model_layers_11_self_attn_q_proj ==> node.args[3]:32>>>> self.tpl.tp_mesh.size(): 2

>> >> model_layers_5_self_attn_k_proj ===> node.args[3]:4
>> >> model_layers_5_self_attn_k_proj ===> node.args[3]:4
>model_layers_9_self_attn_q_proj ==> node.args[3]:32> >model_layers_11_self_attn_q_proj ==> node.args[3]:16
>>>model_layers_5_self_attn_v_proj ===> node.args[3]:8

>>>> self.tpl.tp_mesh.size(): 2>>>model_layers_5_self_attn_v_proj ===> node.args[3]:8
>>model_layers_11_self_attn_k_proj ===> node.args[3]:8


>>> >>>model_layers_5_self_attn_v_proj ===> node.args[3]:4
>>> >>>model_layers_5_self_attn_v_proj ===> node.args[3]:4>> >> model_layers_11_self_attn_k_proj ===> node.args[3]:4

>>>model_layers_11_self_attn_v_proj ===> node.args[3]:8> >model_layers_9_self_attn_q_proj ==> node.args[3]:16

>model_layers_6_self_attn_q_proj ==> node.args[3]:32>>> >>>model_layers_11_self_attn_v_proj ===> node.args[3]:4>>model_layers_9_self_attn_k_proj ===> node.args[3]:8

>model_layers_6_self_attn_q_proj ==> node.args[3]:32

> >model_layers_6_self_attn_q_proj ==> node.args[3]:16
>> >> model_layers_9_self_attn_k_proj ===> node.args[3]:4> >model_layers_6_self_attn_q_proj ==> node.args[3]:16>>model_layers_6_self_attn_k_proj ===> node.args[3]:8>model_layers_12_self_attn_q_proj ==> node.args[3]:32



>>>model_layers_9_self_attn_v_proj ===> node.args[3]:8>>model_layers_6_self_attn_k_proj ===> node.args[3]:8> >model_layers_12_self_attn_q_proj ==> node.args[3]:16
>> >> model_layers_6_self_attn_k_proj ===> node.args[3]:4>model_layers_0_self_attn_q_proj ==> node.args[3]:32


>>> >>>model_layers_9_self_attn_v_proj ===> node.args[3]:4
>>model_layers_12_self_attn_k_proj ===> node.args[3]:8>> >> model_layers_6_self_attn_k_proj ===> node.args[3]:4>>>model_layers_6_self_attn_v_proj ===> node.args[3]:8



>>>model_layers_6_self_attn_v_proj ===> node.args[3]:8>> >> model_layers_12_self_attn_k_proj ===> node.args[3]:4
>>> >>>model_layers_6_self_attn_v_proj ===> node.args[3]:4
>model_layers_10_self_attn_q_proj ==> node.args[3]:32
>>> >>>model_layers_6_self_attn_v_proj ===> node.args[3]:4>>>model_layers_12_self_attn_v_proj ===> node.args[3]:8


> >model_layers_10_self_attn_q_proj ==> node.args[3]:16
>>> >>>model_layers_12_self_attn_v_proj ===> node.args[3]:4>>model_layers_10_self_attn_k_proj ===> node.args[3]:8
>model_layers_7_self_attn_q_proj ==> node.args[3]:32>model_layers_0_self_attn_q_proj ==> node.args[3]:32

>model_layers_7_self_attn_q_proj ==> node.args[3]:32
>> >> model_layers_10_self_attn_k_proj ===> node.args[3]:4
>model_layers_13_self_attn_q_proj ==> node.args[3]:32> >model_layers_7_self_attn_q_proj ==> node.args[3]:16

> >model_layers_0_self_attn_q_proj ==> node.args[3]:16> >model_layers_7_self_attn_q_proj ==> node.args[3]:16
>>>model_layers_10_self_attn_v_proj ===> node.args[3]:8
> >model_layers_13_self_attn_q_proj ==> node.args[3]:16>>model_layers_0_self_attn_k_proj ===> node.args[3]:8
>>model_layers_7_self_attn_k_proj ===> node.args[3]:8


>>model_layers_7_self_attn_k_proj ===> node.args[3]:8
>>model_layers_13_self_attn_k_proj ===> node.args[3]:8>>> >>>model_layers_10_self_attn_v_proj ===> node.args[3]:4
>> >> model_layers_0_self_attn_k_proj ===> node.args[3]:4

>> >> model_layers_7_self_attn_k_proj ===> node.args[3]:4
>> >> model_layers_7_self_attn_k_proj ===> node.args[3]:4>> >> model_layers_13_self_attn_k_proj ===> node.args[3]:4
>>>model_layers_0_self_attn_v_proj ===> node.args[3]:8

>>>model_layers_7_self_attn_v_proj ===> node.args[3]:8
>model_layers_11_self_attn_q_proj ==> node.args[3]:32>>>model_layers_7_self_attn_v_proj ===> node.args[3]:8>>>model_layers_13_self_attn_v_proj ===> node.args[3]:8

> >model_layers_0_self_attn_q_proj ==> node.args[3]:16>>> >>>model_layers_7_self_attn_v_proj ===> node.args[3]:4>>> >>>model_layers_0_self_attn_v_proj ===> node.args[3]:4


> >model_layers_11_self_attn_q_proj ==> node.args[3]:16

>>> >>>model_layers_7_self_attn_v_proj ===> node.args[3]:4>>> >>>model_layers_13_self_attn_v_proj ===> node.args[3]:4>>model_layers_0_self_attn_k_proj ===> node.args[3]:8



>>model_layers_11_self_attn_k_proj ===> node.args[3]:8>model_layers_8_self_attn_q_proj ==> node.args[3]:32>model_layers_1_self_attn_q_proj ==> node.args[3]:32

>model_layers_8_self_attn_q_proj ==> node.args[3]:32
>> >> model_layers_0_self_attn_k_proj ===> node.args[3]:4>> >> model_layers_11_self_attn_k_proj ===> node.args[3]:4
> >model_layers_8_self_attn_q_proj ==> node.args[3]:16

> >model_layers_1_self_attn_q_proj ==> node.args[3]:16
> >model_layers_8_self_attn_q_proj ==> node.args[3]:16>>>model_layers_0_self_attn_v_proj ===> node.args[3]:8>>>model_layers_11_self_attn_v_proj ===> node.args[3]:8
>>model_layers_8_self_attn_k_proj ===> node.args[3]:8


>>model_layers_1_self_attn_k_proj ===> node.args[3]:8
>>model_layers_8_self_attn_k_proj ===> node.args[3]:8
>>> >>>model_layers_0_self_attn_v_proj ===> node.args[3]:4>>> >>>model_layers_11_self_attn_v_proj ===> node.args[3]:4
>> >> model_layers_8_self_attn_k_proj ===> node.args[3]:4

>> >> model_layers_1_self_attn_k_proj ===> node.args[3]:4
>> >> model_layers_8_self_attn_k_proj ===> node.args[3]:4
>>>model_layers_8_self_attn_v_proj ===> node.args[3]:8
>>>model_layers_1_self_attn_v_proj ===> node.args[3]:8>model_layers_1_self_attn_q_proj ==> node.args[3]:32
>model_layers_12_self_attn_q_proj ==> node.args[3]:32>>>model_layers_8_self_attn_v_proj ===> node.args[3]:8


>>> >>>model_layers_8_self_attn_v_proj ===> node.args[3]:4
>>> >>>model_layers_1_self_attn_v_proj ===> node.args[3]:4
> >model_layers_1_self_attn_q_proj ==> node.args[3]:16> >model_layers_12_self_attn_q_proj ==> node.args[3]:16
>>> >>>model_layers_8_self_attn_v_proj ===> node.args[3]:4


>>model_layers_1_self_attn_k_proj ===> node.args[3]:8>>model_layers_12_self_attn_k_proj ===> node.args[3]:8

>> >> model_layers_1_self_attn_k_proj ===> node.args[3]:4
>> >> model_layers_12_self_attn_k_proj ===> node.args[3]:4>>>model_layers_1_self_attn_v_proj ===> node.args[3]:8

>>>model_layers_12_self_attn_v_proj ===> node.args[3]:8>model_layers_2_self_attn_q_proj ==> node.args[3]:32
 >> rank:6 -----------------------------------------------
>>> >>>model_layers_1_self_attn_v_proj ===> node.args[3]:4>>> >>>model_layers_12_self_attn_v_proj ===> node.args[3]:4

> >model_layers_2_self_attn_q_proj ==> node.args[3]:16

rank: 6, #### last layer id:15>>model_layers_2_self_attn_k_proj ===> node.args[3]:8

>model_layers_13_self_attn_q_proj ==> node.args[3]:32 >> rank:6 -----------------------------------------------
>model_layers_2_self_attn_q_proj ==> node.args[3]:32>> >> model_layers_2_self_attn_k_proj ===> node.args[3]:4

> >model_layers_13_self_attn_q_proj ==> node.args[3]:16

>>>model_layers_2_self_attn_v_proj ===> node.args[3]:8>>model_layers_13_self_attn_k_proj ===> node.args[3]:8> >model_layers_2_self_attn_q_proj ==> node.args[3]:16


>>> >>>model_layers_2_self_attn_v_proj ===> node.args[3]:4>>model_layers_2_self_attn_k_proj ===> node.args[3]:8>> >> model_layers_13_self_attn_k_proj ===> node.args[3]:4


>>>model_layers_13_self_attn_v_proj ===> node.args[3]:8
>> >> model_layers_2_self_attn_k_proj ===> node.args[3]:4
>>> >>>model_layers_13_self_attn_v_proj ===> node.args[3]:4>>>model_layers_2_self_attn_v_proj ===> node.args[3]:8

>model_layers_3_self_attn_q_proj ==> node.args[3]:32
>>> >>>model_layers_2_self_attn_v_proj ===> node.args[3]:4
> >model_layers_3_self_attn_q_proj ==> node.args[3]:16
>>model_layers_3_self_attn_k_proj ===> node.args[3]:8
>> >> model_layers_3_self_attn_k_proj ===> node.args[3]:4
>>>model_layers_3_self_attn_v_proj ===> node.args[3]:8>model_layers_3_self_attn_q_proj ==> node.args[3]:32

>>> >>>model_layers_3_self_attn_v_proj ===> node.args[3]:4
> >model_layers_3_self_attn_q_proj ==> node.args[3]:16>>>> self.tpl.tp_mesh.size(): 2

>>model_layers_3_self_attn_k_proj ===> node.args[3]:8
>> >> model_layers_3_self_attn_k_proj ===> node.args[3]:4
>>>model_layers_3_self_attn_v_proj ===> node.args[3]:8
>>> >>>model_layers_3_self_attn_v_proj ===> node.args[3]:4
>model_layers_14_self_attn_q_proj ==> node.args[3]:32
> >model_layers_14_self_attn_q_proj ==> node.args[3]:16
>>model_layers_14_self_attn_k_proj ===> node.args[3]:8
>> >> model_layers_14_self_attn_k_proj ===> node.args[3]:4
>>>model_layers_14_self_attn_v_proj ===> node.args[3]:8
>>> >>>model_layers_14_self_attn_v_proj ===> node.args[3]:4
>model_layers_15_self_attn_q_proj ==> node.args[3]:32
> >model_layers_15_self_attn_q_proj ==> node.args[3]:16
>>model_layers_15_self_attn_k_proj ===> node.args[3]:8
>> >> model_layers_15_self_attn_k_proj ===> node.args[3]:4
>>>model_layers_15_self_attn_v_proj ===> node.args[3]:8
>>> >>>model_layers_15_self_attn_v_proj ===> node.args[3]:4
 *********** rank:1 cross-referenced nodes *****************
   special_nodes: {'getitem_2': (1, 3), 'getitem_3': (1, 3), 'getitem_4': (1, 3), 'submod_2': (2, 3), 'submod_1': (1, 2), 'submod_0': (0, 1)}
 *************************************************************************
 *********** rank:0 cross-referenced nodes *****************
   special_nodes: {'getitem_2': (1, 3), 'getitem_3': (1, 3), 'getitem_4': (1, 3), 'submod_2': (2, 3), 'submod_1': (1, 2), 'submod_0': (0, 1)}
 *************************************************************************
 rank=0 ...
 *********** rank:3 cross-referenced nodes *****************
   special_nodes: {'getitem_2': (1, 3), 'getitem_3': (1, 3), 'getitem_4': (1, 3), 'submod_2': (2, 3), 'submod_1': (1, 2), 'submod_0': (0, 1)}
 *************************************************************************
 *********** rank:2 cross-referenced nodes *****************
   special_nodes: {'getitem_2': (1, 3), 'getitem_3': (1, 3), 'getitem_4': (1, 3), 'submod_2': (2, 3), 'submod_1': (1, 2), 'submod_0': (0, 1)}
 *************************************************************************
 *********** rank:4 cross-referenced nodes *****************
   special_nodes: {'getitem_2': (1, 3), 'getitem_3': (1, 3), 'getitem_4': (1, 3), 'submod_2': (2, 3), 'submod_1': (1, 2), 'submod_0': (0, 1)}
 *************************************************************************
 *********** rank:5 cross-referenced nodes *****************
   special_nodes: {'getitem_2': (1, 3), 'getitem_3': (1, 3), 'getitem_4': (1, 3), 'submod_2': (2, 3), 'submod_1': (1, 2), 'submod_0': (0, 1)}
 *************************************************************************
 *********** rank:7 cross-referenced nodes *****************
   special_nodes: {'getitem_2': (1, 3), 'getitem_3': (1, 3), 'getitem_4': (1, 3), 'submod_2': (2, 3), 'submod_1': (1, 2), 'submod_0': (0, 1)}
 *************************************************************************
 *********** rank:6 cross-referenced nodes *****************
   special_nodes: {'getitem_2': (1, 3), 'getitem_3': (1, 3), 'getitem_4': (1, 3), 'submod_2': (2, 3), 'submod_1': (1, 2), 'submod_0': (0, 1)}
 *************************************************************************
data_size=10334
nbatches=323
[rank:6 stage:3] transformer-block(module) forward time (no send/recv) - step 1
  layer[14]: total=175.268ms, ops=8, avg/op=21.908ms
  layer[15]: total=8.987ms, ops=8, avg/op=1.123ms
  layer[lm_head]: total=75.692ms, ops=1, avg/op=75.692ms
[rank:4 stage:2] transformer-block(module) forward time (no send/recv) - step 1
  layer[9]: total=154.059ms, ops=8, avg/op=19.257ms
  layer[10]: total=9.346ms, ops=8, avg/op=1.168ms
  layer[11]: total=8.985ms, ops=8, avg/op=1.123ms
  layer[12]: total=8.991ms, ops=8, avg/op=1.124ms
  layer[13]: total=9.082ms, ops=8, avg/op=1.135ms
/opt/conda/lib/python3.11/site-packages/torch/autograd/graph.py:825: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at ../aten/src/ATen/cuda/CublasHandlePool.cpp:135.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank:2 stage:1] transformer-block(module) forward time (no send/recv) - step 1
  layer[4]: total=108.777ms, ops=8, avg/op=13.597ms
  layer[5]: total=8.852ms, ops=8, avg/op=1.107ms
  layer[6]: total=8.869ms, ops=8, avg/op=1.109ms
  layer[7]: total=8.875ms, ops=8, avg/op=1.109ms
  layer[8]: total=8.941ms, ops=8, avg/op=1.118ms
[rank:0 stage:0] transformer-block(module) forward time (no send/recv) - step 1
  layer[embed]: total=76.905ms, ops=1, avg/op=76.905ms
  layer[0]: total=34.092ms, ops=8, avg/op=4.261ms
  layer[1]: total=9.196ms, ops=8, avg/op=1.149ms
  layer[2]: total=9.222ms, ops=8, avg/op=1.153ms
  layer[3]: total=9.297ms, ops=8, avg/op=1.162ms
[rank:6 stage:3] transformer-block(module) forward time (no send/recv) - step 2
  layer[14]: total=15.920ms, ops=8, avg/op=1.990ms
  layer[15]: total=10.992ms, ops=8, avg/op=1.374ms
  layer[lm_head]: total=99.732ms, ops=1, avg/op=99.732ms
| epoch   1 |     1/  323 batches | lr 0.00 | ms/batch 4118.83 | loss 25.06 | ppl 76428232195.66
| epoch   1 |     1/  323 batches | lr 0.00 | ms/batch 4578.41 | loss 25.06 | ppl 76428232195.66
[rank:4 stage:2] transformer-block(module) forward time (no send/recv) - step 2
  layer[9]: total=14.880ms, ops=8, avg/op=1.860ms
  layer[10]: total=10.970ms, ops=8, avg/op=1.371ms
  layer[11]: total=11.083ms, ops=8, avg/op=1.385ms
  layer[12]: total=11.150ms, ops=8, avg/op=1.394ms
  layer[13]: total=11.128ms, ops=8, avg/op=1.391ms
[rank:2 stage:1] transformer-block(module) forward time (no send/recv) - step 2
  layer[4]: total=16.811ms, ops=8, avg/op=2.101ms
  layer[5]: total=10.868ms, ops=8, avg/op=1.358ms
  layer[6]: total=10.886ms, ops=8, avg/op=1.361ms
  layer[7]: total=11.006ms, ops=8, avg/op=1.376ms
  layer[8]: total=11.286ms, ops=8, avg/op=1.411ms
[rank:0 stage:0] transformer-block(module) forward time (no send/recv) - step 2
  layer[embed]: total=0.147ms, ops=1, avg/op=0.147ms
  layer[0]: total=17.041ms, ops=8, avg/op=2.130ms
  layer[1]: total=11.293ms, ops=8, avg/op=1.412ms
  layer[2]: total=11.331ms, ops=8, avg/op=1.416ms
  layer[3]: total=11.398ms, ops=8, avg/op=1.425ms
[rank:6 stage:3] transformer-block(module) forward time (no send/recv) - step 3
  layer[14]: total=11.028ms, ops=8, avg/op=1.379ms
  layer[15]: total=11.008ms, ops=8, avg/op=1.376ms
  layer[lm_head]: total=99.863ms, ops=1, avg/op=99.863ms
| epoch   1 |     2/  323 batches | lr 0.00 | ms/batch 1152.11 | loss 12.40 | ppl 241901.76
| epoch   1 |     2/  323 batches | lr 0.00 | ms/batch 1271.13 | loss 12.40 | ppl 241901.76
[rank:4 stage:2] transformer-block(module) forward time (no send/recv) - step 3
  layer[9]: total=10.994ms, ops=8, avg/op=1.374ms
  layer[10]: total=11.000ms, ops=8, avg/op=1.375ms
  layer[11]: total=11.054ms, ops=8, avg/op=1.382ms
  layer[12]: total=11.151ms, ops=8, avg/op=1.394ms
  layer[13]: total=11.197ms, ops=8, avg/op=1.400ms
[rank:0 stage:0] transformer-block(module) forward time (no send/recv) - step 3
  layer[embed]: total=0.159ms, ops=1, avg/op=0.159ms
  layer[0]: total=11.306ms, ops=8, avg/op=1.413ms
  layer[1]: total=11.208ms, ops=8, avg/op=1.401ms
  layer[2]: total=11.206ms, ops=8, avg/op=1.401ms
  layer[3]: total=11.340ms, ops=8, avg/op=1.417ms
[rank:2 stage:1] transformer-block(module) forward time (no send/recv) - step 3
  layer[4]: total=11.078ms, ops=8, avg/op=1.385ms
  layer[5]: total=10.863ms, ops=8, avg/op=1.358ms
  layer[6]: total=11.021ms, ops=8, avg/op=1.378ms
  layer[7]: total=11.053ms, ops=8, avg/op=1.382ms
  layer[8]: total=11.080ms, ops=8, avg/op=1.385ms
[rank:6 stage:3] transformer-block(module) forward time (no send/recv) - step 4
  layer[14]: total=16.181ms, ops=8, avg/op=2.023ms
  layer[15]: total=12.114ms, ops=8, avg/op=1.514ms
  layer[lm_head]: total=107.260ms, ops=1, avg/op=107.260ms
| epoch   1 |     3/  323 batches | lr 0.00 | ms/batch 1118.13 | loss 13.02 | ppl 449416.39
| epoch   1 |     3/  323 batches | lr 0.00 | ms/batch 1131.27 | loss 13.02 | ppl 449416.39
[rank:4 stage:2] transformer-block(module) forward time (no send/recv) - step 4
  layer[9]: total=15.809ms, ops=8, avg/op=1.976ms
  layer[10]: total=12.129ms, ops=8, avg/op=1.516ms
  layer[11]: total=12.188ms, ops=8, avg/op=1.523ms
  layer[12]: total=12.243ms, ops=8, avg/op=1.530ms
  layer[13]: total=12.326ms, ops=8, avg/op=1.541ms
[rank:2 stage:1] transformer-block(module) forward time (no send/recv) - step 4
  layer[4]: total=17.333ms, ops=8, avg/op=2.167ms
  layer[5]: total=12.149ms, ops=8, avg/op=1.519ms
  layer[6]: total=12.044ms, ops=8, avg/op=1.506ms
  layer[7]: total=12.068ms, ops=8, avg/op=1.508ms
  layer[8]: total=12.155ms, ops=8, avg/op=1.519ms
[rank:0 stage:0] transformer-block(module) forward time (no send/recv) - step 4
  layer[embed]: total=0.178ms, ops=1, avg/op=0.178ms
  layer[0]: total=18.938ms, ops=8, avg/op=2.367ms
  layer[1]: total=12.332ms, ops=8, avg/op=1.541ms
  layer[2]: total=12.405ms, ops=8, avg/op=1.551ms
  layer[3]: total=12.485ms, ops=8, avg/op=1.561ms
[rank:6 stage:3] transformer-block(module) forward time (no send/recv) - step 5
  layer[14]: total=16.392ms, ops=8, avg/op=2.049ms
  layer[15]: total=12.528ms, ops=8, avg/op=1.566ms
  layer[lm_head]: total=110.924ms, ops=1, avg/op=110.924ms
| epoch   1 |     4/  323 batches | lr 0.00 | ms/batch 1185.57 | loss 13.53 | ppl 752117.79
| epoch   1 |     4/  323 batches | lr 0.00 | ms/batch 1147.16 | loss 13.53 | ppl 752117.79
[rank:4 stage:2] transformer-block(module) forward time (no send/recv) - step 5
  layer[9]: total=16.348ms, ops=8, avg/op=2.044ms
  layer[10]: total=12.539ms, ops=8, avg/op=1.567ms
  layer[11]: total=12.612ms, ops=8, avg/op=1.576ms
  layer[12]: total=12.716ms, ops=8, avg/op=1.590ms
  layer[13]: total=12.903ms, ops=8, avg/op=1.613ms
[rank:2 stage:1] transformer-block(module) forward time (no send/recv) - step 5
  layer[4]: total=18.606ms, ops=8, avg/op=2.326ms
  layer[5]: total=12.394ms, ops=8, avg/op=1.549ms
  layer[6]: total=12.461ms, ops=8, avg/op=1.558ms
  layer[7]: total=12.680ms, ops=8, avg/op=1.585ms
  layer[8]: total=13.007ms, ops=8, avg/op=1.626ms
[rank:0 stage:0] transformer-block(module) forward time (no send/recv) - step 5
  layer[embed]: total=0.181ms, ops=1, avg/op=0.181ms
  layer[0]: total=21.350ms, ops=8, avg/op=2.669ms
  layer[1]: total=12.781ms, ops=8, avg/op=1.598ms
  layer[2]: total=12.880ms, ops=8, avg/op=1.610ms
  layer[3]: total=13.071ms, ops=8, avg/op=1.634ms
| epoch   1 |     5/  323 batches | lr 0.00 | ms/batch 1053.84 | loss 13.34 | ppl 621168.32
[rank:6 stage:3] transformer-block(module) forward time (no send/recv) - step 6
  layer[14]: total=16.830ms, ops=8, avg/op=2.104ms
  layer[15]: total=11.898ms, ops=8, avg/op=1.487ms
  layer[lm_head]: total=103.137ms, ops=1, avg/op=103.137ms
| epoch   1 |     5/  323 batches | lr 0.00 | ms/batch 1166.95 | loss 13.34 | ppl 621168.32
[rank:4 stage:2] transformer-block(module) forward time (no send/recv) - step 6
  layer[9]: total=15.530ms, ops=8, avg/op=1.941ms
  layer[10]: total=11.867ms, ops=8, avg/op=1.483ms
  layer[11]: total=11.986ms, ops=8, avg/op=1.498ms
  layer[12]: total=12.129ms, ops=8, avg/op=1.516ms
  layer[13]: total=12.185ms, ops=8, avg/op=1.523ms
[rank:2 stage:1] transformer-block(module) forward time (no send/recv) - step 6
  layer[4]: total=17.229ms, ops=8, avg/op=2.154ms
  layer[5]: total=11.771ms, ops=8, avg/op=1.471ms
  layer[6]: total=11.837ms, ops=8, avg/op=1.480ms
  layer[7]: total=12.007ms, ops=8, avg/op=1.501ms
  layer[8]: total=12.271ms, ops=8, avg/op=1.534ms
[rank:0 stage:0] transformer-block(module) forward time (no send/recv) - step 6
  layer[embed]: total=0.369ms, ops=1, avg/op=0.369ms
  layer[0]: total=18.173ms, ops=8, avg/op=2.272ms
  layer[1]: total=12.159ms, ops=8, avg/op=1.520ms
  layer[2]: total=12.269ms, ops=8, avg/op=1.534ms
  layer[3]: total=12.328ms, ops=8, avg/op=1.541ms
[rank:6 stage:3] transformer-block(module) forward time (no send/recv) - step 7
  layer[14]: total=15.671ms, ops=8, avg/op=1.959ms
  layer[15]: total=10.982ms, ops=8, avg/op=1.373ms
  layer[lm_head]: total=95.705ms, ops=1, avg/op=95.705ms
| epoch   1 |     6/  323 batches | lr 0.00 | ms/batch 1093.27 | loss 13.58 | ppl 790436.33
| epoch   1 |     6/  323 batches | lr 0.00 | ms/batch 1107.84 | loss 13.58 | ppl 790436.33
[rank:4 stage:2] transformer-block(module) forward time (no send/recv) - step 7
  layer[9]: total=14.796ms, ops=8, avg/op=1.849ms
  layer[10]: total=10.980ms, ops=8, avg/op=1.373ms
  layer[11]: total=11.018ms, ops=8, avg/op=1.377ms
  layer[12]: total=11.115ms, ops=8, avg/op=1.389ms
  layer[13]: total=11.182ms, ops=8, avg/op=1.398ms
[rank:2 stage:1] transformer-block(module) forward time (no send/recv) - step 7
  layer[4]: total=16.708ms, ops=8, avg/op=2.089ms
  layer[5]: total=10.864ms, ops=8, avg/op=1.358ms
  layer[6]: total=10.977ms, ops=8, avg/op=1.372ms
  layer[7]: total=11.028ms, ops=8, avg/op=1.379ms
  layer[8]: total=11.001ms, ops=8, avg/op=1.375ms
[rank:0 stage:0] transformer-block(module) forward time (no send/recv) - step 7
  layer[embed]: total=0.152ms, ops=1, avg/op=0.152ms
  layer[0]: total=20.144ms, ops=8, avg/op=2.518ms
  layer[1]: total=11.180ms, ops=8, avg/op=1.398ms
  layer[2]: total=11.258ms, ops=8, avg/op=1.407ms
  layer[3]: total=11.343ms, ops=8, avg/op=1.418ms
| epoch   1 |     7/  323 batches | lr 0.00 | ms/batch 1020.93 | loss 13.47 | ppl 704372.17
[rank:6 stage:3] transformer-block(module) forward time (no send/recv) - step 8
  layer[14]: total=15.597ms, ops=8, avg/op=1.950ms
  layer[15]: total=9.861ms, ops=8, avg/op=1.233ms
  layer[lm_head]: total=83.877ms, ops=1, avg/op=83.877ms
| epoch   1 |     7/  323 batches | lr 0.00 | ms/batch 1038.97 | loss 13.47 | ppl 704372.17
[rank:4 stage:2] transformer-block(module) forward time (no send/recv) - step 8
  layer[9]: total=14.495ms, ops=8, avg/op=1.812ms
  layer[10]: total=9.859ms, ops=8, avg/op=1.232ms
  layer[11]: total=9.940ms, ops=8, avg/op=1.243ms
  layer[12]: total=9.979ms, ops=8, avg/op=1.247ms
  layer[13]: total=10.022ms, ops=8, avg/op=1.253ms
[rank:2 stage:1] transformer-block(module) forward time (no send/recv) - step 8
  layer[4]: total=16.204ms, ops=8, avg/op=2.025ms
  layer[5]: total=9.780ms, ops=8, avg/op=1.223ms
  layer[6]: total=9.839ms, ops=8, avg/op=1.230ms
  layer[7]: total=9.866ms, ops=8, avg/op=1.233ms
  layer[8]: total=10.076ms, ops=8, avg/op=1.260ms
[rank:0 stage:0] transformer-block(module) forward time (no send/recv) - step 8
  layer[embed]: total=0.148ms, ops=1, avg/op=0.148ms
  layer[0]: total=16.483ms, ops=8, avg/op=2.060ms
  layer[1]: total=10.029ms, ops=8, avg/op=1.254ms
  layer[2]: total=10.203ms, ops=8, avg/op=1.275ms
  layer[3]: total=10.127ms, ops=8, avg/op=1.266ms
[rank:6 stage:3] transformer-block(module) forward time (no send/recv) - step 9
  layer[14]: total=16.352ms, ops=8, avg/op=2.044ms
  layer[15]: total=12.387ms, ops=8, avg/op=1.548ms
  layer[lm_head]: total=112.727ms, ops=1, avg/op=112.727ms
| epoch   1 |     8/  323 batches | lr 0.00 | ms/batch 1117.53 | loss 13.26 | ppl 572336.04
| epoch   1 |     8/  323 batches | lr 0.00 | ms/batch 1146.12 | loss 13.26 | ppl 572336.04
[rank:4 stage:2] transformer-block(module) forward time (no send/recv) - step 9
  layer[9]: total=15.486ms, ops=8, avg/op=1.936ms
  layer[10]: total=12.351ms, ops=8, avg/op=1.544ms
  layer[11]: total=12.508ms, ops=8, avg/op=1.564ms
  layer[12]: total=12.671ms, ops=8, avg/op=1.584ms
  layer[13]: total=12.727ms, ops=8, avg/op=1.591ms
[rank:2 stage:1] transformer-block(module) forward time (no send/recv) - step 9
  layer[4]: total=18.585ms, ops=8, avg/op=2.323ms
  layer[5]: total=12.243ms, ops=8, avg/op=1.530ms
  layer[6]: total=12.415ms, ops=8, avg/op=1.552ms
  layer[7]: total=12.759ms, ops=8, avg/op=1.595ms
  layer[8]: total=12.829ms, ops=8, avg/op=1.604ms
[rank:0 stage:0] transformer-block(module) forward time (no send/recv) - step 9
  layer[embed]: total=0.200ms, ops=1, avg/op=0.200ms
  layer[0]: total=21.681ms, ops=8, avg/op=2.710ms
  layer[1]: total=12.609ms, ops=8, avg/op=1.576ms
  layer[2]: total=12.731ms, ops=8, avg/op=1.591ms
  layer[3]: total=12.731ms, ops=8, avg/op=1.591ms
| epoch   1 |     9/  323 batches | lr 0.00 | ms/batch 1188.40 | loss 13.20 | ppl 542503.56
[rank:6 stage:3] transformer-block(module) forward time (no send/recv) - step 10
  layer[14]: total=12.416ms, ops=8, avg/op=1.552ms
  layer[15]: total=12.420ms, ops=8, avg/op=1.553ms
  layer[lm_head]: total=113.770ms, ops=1, avg/op=113.770ms
| epoch   1 |     9/  323 batches | lr 0.00 | ms/batch 1210.98 | loss 13.20 | ppl 542503.56
[rank:4 stage:2] transformer-block(module) forward time (no send/recv) - step 10
  layer[9]: total=17.440ms, ops=8, avg/op=2.180ms
  layer[10]: total=12.412ms, ops=8, avg/op=1.551ms
  layer[11]: total=12.417ms, ops=8, avg/op=1.552ms
  layer[12]: total=12.484ms, ops=8, avg/op=1.560ms
  layer[13]: total=12.652ms, ops=8, avg/op=1.581ms
[rank:2 stage:1] transformer-block(module) forward time (no send/recv) - step 10
  layer[4]: total=12.501ms, ops=8, avg/op=1.563ms
  layer[5]: total=12.140ms, ops=8, avg/op=1.517ms
  layer[6]: total=12.173ms, ops=8, avg/op=1.522ms
  layer[7]: total=12.202ms, ops=8, avg/op=1.525ms
  layer[8]: total=12.382ms, ops=8, avg/op=1.548ms
[rank:0 stage:0] transformer-block(module) forward time (no send/recv) - step 10
  layer[embed]: total=0.187ms, ops=1, avg/op=0.187ms
  layer[0]: total=13.405ms, ops=8, avg/op=1.676ms
  layer[1]: total=12.678ms, ops=8, avg/op=1.585ms
  layer[2]: total=12.740ms, ops=8, avg/op=1.592ms
  layer[3]: total=12.784ms, ops=8, avg/op=1.598ms
[rank:6 stage:3] transformer-block(module) forward time (no send/recv) - step 11
  layer[14]: total=16.440ms, ops=8, avg/op=2.055ms
  layer[15]: total=12.384ms, ops=8, avg/op=1.548ms
  layer[lm_head]: total=112.220ms, ops=1, avg/op=112.220ms
| epoch   1 |    10/  323 batches | lr 0.00 | ms/batch 1159.12 | loss 13.03 | ppl 454741.72
| epoch   1 |    10/  323 batches | lr 0.00 | ms/batch 1183.80 | loss 13.03 | ppl 454741.72
[rank:4 stage:2] transformer-block(module) forward time (no send/recv) - step 11
  layer[9]: total=15.782ms, ops=8, avg/op=1.973ms
  layer[10]: total=12.353ms, ops=8, avg/op=1.544ms
  layer[11]: total=12.457ms, ops=8, avg/op=1.557ms
  layer[12]: total=12.545ms, ops=8, avg/op=1.568ms
  layer[13]: total=12.571ms, ops=8, avg/op=1.571ms
[rank:2 stage:1] transformer-block(module) forward time (no send/recv) - step 11
  layer[4]: total=17.742ms, ops=8, avg/op=2.218ms
  layer[5]: total=12.111ms, ops=8, avg/op=1.514ms
  layer[6]: total=12.235ms, ops=8, avg/op=1.529ms
  layer[7]: total=12.312ms, ops=8, avg/op=1.539ms
  layer[8]: total=12.380ms, ops=8, avg/op=1.548ms
[rank:0 stage:0] transformer-block(module) forward time (no send/recv) - step 11
  layer[embed]: total=0.171ms, ops=1, avg/op=0.171ms
  layer[0]: total=17.999ms, ops=8, avg/op=2.250ms
  layer[1]: total=12.665ms, ops=8, avg/op=1.583ms
  layer[2]: total=12.695ms, ops=8, avg/op=1.587ms
  layer[3]: total=12.786ms, ops=8, avg/op=1.598ms
| epoch   1 |    11/  323 batches | lr 0.00 | ms/batch 1161.71 | loss 12.70 | ppl 329171.22
[rank:6 stage:3] transformer-block(module) forward time (no send/recv) - step 12
  layer[14]: total=12.411ms, ops=8, avg/op=1.551ms
  layer[15]: total=12.363ms, ops=8, avg/op=1.545ms
  layer[lm_head]: total=113.104ms, ops=1, avg/op=113.104ms
| epoch   1 |    11/  323 batches | lr 0.00 | ms/batch 1189.56 | loss 12.70 | ppl 329171.22
[rank:4 stage:2] transformer-block(module) forward time (no send/recv) - step 12
  layer[9]: total=12.368ms, ops=8, avg/op=1.546ms
  layer[10]: total=12.373ms, ops=8, avg/op=1.547ms
  layer[11]: total=12.411ms, ops=8, avg/op=1.551ms
  layer[12]: total=12.447ms, ops=8, avg/op=1.556ms
  layer[13]: total=12.572ms, ops=8, avg/op=1.571ms
[rank:2 stage:1] transformer-block(module) forward time (no send/recv) - step 12
  layer[4]: total=12.270ms, ops=8, avg/op=1.534ms
  layer[5]: total=12.109ms, ops=8, avg/op=1.514ms
  layer[6]: total=12.260ms, ops=8, avg/op=1.533ms
  layer[7]: total=12.306ms, ops=8, avg/op=1.538ms
  layer[8]: total=12.425ms, ops=8, avg/op=1.553ms
[rank:0 stage:0] transformer-block(module) forward time (no send/recv) - step 12
  layer[embed]: total=0.158ms, ops=1, avg/op=0.158ms
  layer[0]: total=12.781ms, ops=8, avg/op=1.598ms
  layer[1]: total=12.638ms, ops=8, avg/op=1.580ms
  layer[2]: total=12.704ms, ops=8, avg/op=1.588ms
  layer[3]: total=12.784ms, ops=8, avg/op=1.598ms
| epoch   1 |    12/  323 batches | lr 0.00 | ms/batch 1060.55 | loss 12.23 | ppl 204094.11
[rank:6 stage:3] transformer-block(module) forward time (no send/recv) - step 13
  layer[14]: total=11.020ms, ops=8, avg/op=1.378ms
  layer[15]: total=10.980ms, ops=8, avg/op=1.373ms
  layer[lm_head]: total=95.618ms, ops=1, avg/op=95.618ms
| epoch   1 |    12/  323 batches | lr 0.00 | ms/batch 1052.59 | loss 12.23 | ppl 204094.11
[rank:4 stage:2] transformer-block(module) forward time (no send/recv) - step 13
  layer[9]: total=10.885ms, ops=8, avg/op=1.361ms
  layer[10]: total=10.873ms, ops=8, avg/op=1.359ms
  layer[11]: total=10.978ms, ops=8, avg/op=1.372ms
  layer[12]: total=11.019ms, ops=8, avg/op=1.377ms
  layer[13]: total=11.013ms, ops=8, avg/op=1.377ms
[rank:2 stage:1] transformer-block(module) forward time (no send/recv) - step 13
  layer[4]: total=10.946ms, ops=8, avg/op=1.368ms
  layer[5]: total=10.754ms, ops=8, avg/op=1.344ms
  layer[6]: total=10.770ms, ops=8, avg/op=1.346ms
  layer[7]: total=10.795ms, ops=8, avg/op=1.349ms
  layer[8]: total=11.157ms, ops=8, avg/op=1.395ms
[rank:0 stage:0] transformer-block(module) forward time (no send/recv) - step 13
  layer[embed]: total=0.178ms, ops=1, avg/op=0.178ms
  layer[0]: total=11.365ms, ops=8, avg/op=1.421ms
  layer[1]: total=11.203ms, ops=8, avg/op=1.400ms
  layer[2]: total=11.279ms, ops=8, avg/op=1.410ms
  layer[3]: total=11.318ms, ops=8, avg/op=1.415ms
| epoch   1 |    13/  323 batches | lr 0.00 | ms/batch 988.89 | loss 12.24 | ppl 207489.30
[rank:6 stage:3] transformer-block(module) forward time (no send/recv) - step 14
  layer[14]: total=14.135ms, ops=8, avg/op=1.767ms
  layer[15]: total=9.478ms, ops=8, avg/op=1.185ms
  layer[lm_head]: total=84.527ms, ops=1, avg/op=84.527ms
| epoch   1 |    13/  323 batches | lr 0.00 | ms/batch 997.27 | loss 12.24 | ppl 207489.30
[rank:4 stage:2] transformer-block(module) forward time (no send/recv) - step 14
  layer[9]: total=13.570ms, ops=8, avg/op=1.696ms
  layer[10]: total=9.362ms, ops=8, avg/op=1.170ms
  layer[11]: total=9.382ms, ops=8, avg/op=1.173ms
  layer[12]: total=9.418ms, ops=8, avg/op=1.177ms
  layer[13]: total=9.543ms, ops=8, avg/op=1.193ms
[rank:2 stage:1] transformer-block(module) forward time (no send/recv) - step 14
  layer[4]: total=15.082ms, ops=8, avg/op=1.885ms
  layer[5]: total=9.257ms, ops=8, avg/op=1.157ms
  layer[6]: total=9.270ms, ops=8, avg/op=1.159ms
  layer[7]: total=9.313ms, ops=8, avg/op=1.164ms
  layer[8]: total=9.496ms, ops=8, avg/op=1.187ms
[rank:0 stage:0] transformer-block(module) forward time (no send/recv) - step 14
  layer[embed]: total=0.161ms, ops=1, avg/op=0.161ms
  layer[0]: total=15.589ms, ops=8, avg/op=1.949ms
  layer[1]: total=9.599ms, ops=8, avg/op=1.200ms
  layer[2]: total=9.630ms, ops=8, avg/op=1.204ms
  layer[3]: total=9.687ms, ops=8, avg/op=1.211ms
| epoch   1 |    14/  323 batches | lr 0.00 | ms/batch 1082.06 | loss 12.42 | ppl 247644.67
[rank:6 stage:3] transformer-block(module) forward time (no send/recv) - step 15
  layer[14]: total=12.150ms, ops=8, avg/op=1.519ms
  layer[15]: total=12.138ms, ops=8, avg/op=1.517ms
  layer[lm_head]: total=106.894ms, ops=1, avg/op=106.894ms
| epoch   1 |    14/  323 batches | lr 0.00 | ms/batch 1088.31 | loss 12.42 | ppl 247644.67
[rank:4 stage:2] transformer-block(module) forward time (no send/recv) - step 15
  layer[9]: total=11.972ms, ops=8, avg/op=1.496ms
  layer[10]: total=11.991ms, ops=8, avg/op=1.499ms
  layer[11]: total=12.035ms, ops=8, avg/op=1.504ms
  layer[12]: total=12.159ms, ops=8, avg/op=1.520ms
  layer[13]: total=12.413ms, ops=8, avg/op=1.552ms
[rank:2 stage:1] transformer-block(module) forward time (no send/recv) - step 15
  layer[4]: total=12.132ms, ops=8, avg/op=1.517ms
  layer[5]: total=11.857ms, ops=8, avg/op=1.482ms
  layer[6]: total=11.906ms, ops=8, avg/op=1.488ms
  layer[7]: total=11.961ms, ops=8, avg/op=1.495ms
  layer[8]: total=12.176ms, ops=8, avg/op=1.522ms
[rank:0 stage:0] transformer-block(module) forward time (no send/recv) - step 15
  layer[embed]: total=0.161ms, ops=1, avg/op=0.161ms
  layer[0]: total=12.434ms, ops=8, avg/op=1.554ms
  layer[1]: total=12.330ms, ops=8, avg/op=1.541ms
  layer[2]: total=12.380ms, ops=8, avg/op=1.548ms
  layer[3]: total=12.442ms, ops=8, avg/op=1.555ms
[rank:6 stage:3] transformer-block(module) forward time (no send/recv) - step 16
  layer[14]: total=12.149ms, ops=8, avg/op=1.519ms
  layer[15]: total=12.140ms, ops=8, avg/op=1.518ms
  layer[lm_head]: total=106.799ms, ops=1, avg/op=106.799ms
| epoch   1 |    15/  323 batches | lr 0.00 | ms/batch 1105.86 | loss 12.54 | ppl 280476.91
| epoch   1 |    15/  323 batches | lr 0.00 | ms/batch 1252.20 | loss 12.54 | ppl 280476.91
[rank:4 stage:2] transformer-block(module) forward time (no send/recv) - step 16
  layer[9]: total=12.021ms, ops=8, avg/op=1.503ms
  layer[10]: total=12.003ms, ops=8, avg/op=1.500ms
  layer[11]: total=12.036ms, ops=8, avg/op=1.505ms
  layer[12]: total=12.089ms, ops=8, avg/op=1.511ms
  layer[13]: total=12.189ms, ops=8, avg/op=1.524ms
[rank:2 stage:1] transformer-block(module) forward time (no send/recv) - step 16
  layer[4]: total=12.008ms, ops=8, avg/op=1.501ms
  layer[5]: total=11.836ms, ops=8, avg/op=1.480ms
  layer[6]: total=12.058ms, ops=8, avg/op=1.507ms
  layer[7]: total=12.014ms, ops=8, avg/op=1.502ms
  layer[8]: total=12.119ms, ops=8, avg/op=1.515ms
[rank:0 stage:0] transformer-block(module) forward time (no send/recv) - step 16
  layer[embed]: total=0.161ms, ops=1, avg/op=0.161ms
  layer[0]: total=13.143ms, ops=8, avg/op=1.643ms
  layer[1]: total=12.320ms, ops=8, avg/op=1.540ms
  layer[2]: total=12.452ms, ops=8, avg/op=1.556ms
  layer[3]: total=12.516ms, ops=8, avg/op=1.565ms
[rank:6 stage:3] transformer-block(module) forward time (no send/recv) - step 17
  layer[14]: total=14.857ms, ops=8, avg/op=1.857ms
  layer[15]: total=10.326ms, ops=8, avg/op=1.291ms
  layer[lm_head]: total=92.112ms, ops=1, avg/op=92.112ms
| epoch   1 |    16/  323 batches | lr 0.00 | ms/batch 1046.26 | loss 12.62 | ppl 301569.70
[rank:4 stage:2] transformer-block(module) forward time (no send/recv) - step 17
  layer[9]: total=14.208ms, ops=8, avg/op=1.776ms
  layer[10]: total=10.223ms, ops=8, avg/op=1.278ms
  layer[11]: total=10.308ms, ops=8, avg/op=1.288ms
  layer[12]: total=10.354ms, ops=8, avg/op=1.294ms
  layer[13]: total=10.341ms, ops=8, avg/op=1.293ms
| epoch   1 |    16/  323 batches | lr 0.00 | ms/batch 1090.79 | loss 12.62 | ppl 301569.70
[rank:2 stage:1] transformer-block(module) forward time (no send/recv) - step 17
  layer[4]: total=16.312ms, ops=8, avg/op=2.039ms
  layer[5]: total=10.108ms, ops=8, avg/op=1.263ms
  layer[6]: total=10.174ms, ops=8, avg/op=1.272ms
  layer[7]: total=10.209ms, ops=8, avg/op=1.276ms
  layer[8]: total=10.293ms, ops=8, avg/op=1.287ms
[rank:0 stage:0] transformer-block(module) forward time (no send/recv) - step 17
  layer[embed]: total=0.140ms, ops=1, avg/op=0.140ms
  layer[0]: total=17.551ms, ops=8, avg/op=2.194ms
  layer[1]: total=10.478ms, ops=8, avg/op=1.310ms
  layer[2]: total=10.566ms, ops=8, avg/op=1.321ms
  layer[3]: total=10.600ms, ops=8, avg/op=1.325ms
[rank:6 stage:3] transformer-block(module) forward time (no send/recv) - step 18
  layer[14]: total=15.790ms, ops=8, avg/op=1.974ms
  layer[15]: total=10.930ms, ops=8, avg/op=1.366ms
  layer[lm_head]: total=97.568ms, ops=1, avg/op=97.568ms
| epoch   1 |    17/  323 batches | lr 0.00 | ms/batch 1073.98 | loss 12.79 | ppl 358201.44
| epoch   1 |    17/  323 batches | lr 0.00 | ms/batch 986.34 | loss 12.79 | ppl 358201.44
[rank:4 stage:2] transformer-block(module) forward time (no send/recv) - step 18
  layer[9]: total=14.743ms, ops=8, avg/op=1.843ms
  layer[10]: total=10.803ms, ops=8, avg/op=1.350ms
  layer[11]: total=10.829ms, ops=8, avg/op=1.354ms
  layer[12]: total=10.922ms, ops=8, avg/op=1.365ms
  layer[13]: total=10.958ms, ops=8, avg/op=1.370ms
[rank:2 stage:1] transformer-block(module) forward time (no send/recv) - step 18
  layer[4]: total=16.454ms, ops=8, avg/op=2.057ms
  layer[5]: total=10.697ms, ops=8, avg/op=1.337ms
  layer[6]: total=10.808ms, ops=8, avg/op=1.351ms
  layer[7]: total=10.860ms, ops=8, avg/op=1.357ms
  layer[8]: total=10.844ms, ops=8, avg/op=1.356ms
[rank:0 stage:0] transformer-block(module) forward time (no send/recv) - step 18
  layer[embed]: total=0.153ms, ops=1, avg/op=0.153ms
  layer[0]: total=17.786ms, ops=8, avg/op=2.223ms
  layer[1]: total=11.119ms, ops=8, avg/op=1.390ms
  layer[2]: total=11.215ms, ops=8, avg/op=1.402ms
  layer[3]: total=11.260ms, ops=8, avg/op=1.407ms
[rank:6 stage:3] transformer-block(module) forward time (no send/recv) - step 19
  layer[14]: total=15.258ms, ops=8, avg/op=1.907ms
  layer[15]: total=10.251ms, ops=8, avg/op=1.281ms
  layer[lm_head]: total=91.291ms, ops=1, avg/op=91.291ms
| epoch   1 |    18/  323 batches | lr 0.00 | ms/batch 1032.50 | loss 12.94 | ppl 414571.37
| epoch   1 |    18/  323 batches | lr 0.00 | ms/batch 982.70 | loss 12.94 | ppl 414571.37
[rank:4 stage:2] transformer-block(module) forward time (no send/recv) - step 19
  layer[9]: total=15.369ms, ops=8, avg/op=1.921ms
  layer[10]: total=10.175ms, ops=8, avg/op=1.272ms
  layer[11]: total=10.167ms, ops=8, avg/op=1.271ms
  layer[12]: total=10.262ms, ops=8, avg/op=1.283ms
  layer[13]: total=10.288ms, ops=8, avg/op=1.286ms
[rank:2 stage:1] transformer-block(module) forward time (no send/recv) - step 19
  layer[4]: total=16.119ms, ops=8, avg/op=2.015ms
  layer[5]: total=10.053ms, ops=8, avg/op=1.257ms
  layer[6]: total=12.165ms, ops=8, avg/op=1.521ms
  layer[7]: total=10.323ms, ops=8, avg/op=1.290ms
  layer[8]: total=10.171ms, ops=8, avg/op=1.271ms
[rank:0 stage:0] transformer-block(module) forward time (no send/recv) - step 19
  layer[embed]: total=0.156ms, ops=1, avg/op=0.156ms
  layer[0]: total=16.436ms, ops=8, avg/op=2.055ms
  layer[1]: total=10.418ms, ops=8, avg/op=1.302ms
  layer[2]: total=10.444ms, ops=8, avg/op=1.305ms
  layer[3]: total=10.528ms, ops=8, avg/op=1.316ms
[rank:6 stage:3] transformer-block(module) forward time (no send/recv) - step 20
  layer[14]: total=16.015ms, ops=8, avg/op=2.002ms
  layer[15]: total=12.007ms, ops=8, avg/op=1.501ms
  layer[lm_head]: total=106.918ms, ops=1, avg/op=106.918ms
| epoch   1 |    19/  323 batches | lr 0.00 | ms/batch 1124.71 | loss 13.48 | ppl 718310.56
| epoch   1 |    19/  323 batches | lr 0.00 | ms/batch 1113.38 | loss 13.48 | ppl 718310.56
[rank:4 stage:2] transformer-block(module) forward time (no send/recv) - step 20
  layer[9]: total=15.086ms, ops=8, avg/op=1.886ms
  layer[10]: total=11.870ms, ops=8, avg/op=1.484ms
  layer[11]: total=11.901ms, ops=8, avg/op=1.488ms
  layer[12]: total=11.991ms, ops=8, avg/op=1.499ms
  layer[13]: total=12.078ms, ops=8, avg/op=1.510ms
[rank:2 stage:1] transformer-block(module) forward time (no send/recv) - step 20
  layer[4]: total=17.245ms, ops=8, avg/op=2.156ms
  layer[5]: total=11.744ms, ops=8, avg/op=1.468ms
  layer[6]: total=11.845ms, ops=8, avg/op=1.481ms
  layer[7]: total=11.926ms, ops=8, avg/op=1.491ms
  layer[8]: total=12.009ms, ops=8, avg/op=1.501ms
[rank:0 stage:0] transformer-block(module) forward time (no send/recv) - step 20
  layer[embed]: total=0.159ms, ops=1, avg/op=0.159ms
  layer[0]: total=18.169ms, ops=8, avg/op=2.271ms
  layer[1]: total=12.198ms, ops=8, avg/op=1.525ms
  layer[2]: total=12.236ms, ops=8, avg/op=1.529ms
  layer[3]: total=12.338ms, ops=8, avg/op=1.542ms
| epoch   1 |    20/  323 batches | lr 0.00 | ms/batch 1074.84 | loss 12.74 | ppl 339898.56
| epoch   1 |    20/  323 batches | lr 0.00 | ms/batch 1058.74 | loss 12.74 | ppl 339898.56
[rank:6, run completed ...
[rank:7, run completed ...
[rank:4, run completed ...
[rank:5, run completed ...
[rank:2, run completed ...
[rank:3, run completed ...
Time elapsed: 25.564 sec 
[rank:0, run completed ...
[rank:1, run completed ...
[rank0]:[W1224 11:17:55.444699057 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank4]:[W1224 11:17:57.207233637 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank2]:[W1224 11:17:57.339669587 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank6]:[W1224 11:17:57.666944373 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank1]:[W1224 11:17:57.757962618 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank5]:[W1224 11:17:58.125394158 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank3]:[W1224 11:17:58.226243404 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank7]:[W1224 11:17:58.549846828 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
